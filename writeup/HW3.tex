\documentclass[hidelinks]{scrartcl}
\usepackage[final]{nips_2016}
\usepackage{amsmath, amsthm}
\usepackage{amssymb}
\usepackage{color, soul}
\usepackage[margin=0.5in]{geometry}
\usepackage{hyperref}
%% \usepackage[]{mcode}
\usepackage{cancel}
\usepackage{tabu}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{tikz}


\newcommand{\Lbf}[1]{{\noindent \Large{\textbf{#1}}}}
\newcommand{\Part}[1]{\vspace{1cm} {\LARGE\textbf{PART #1}}\\}


%opening
\title{HW3: Deep RL and Controls}



\author{Kenny Marino, Rick Goldstein, Brad Saund, Zhi Tan}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Problem 2}

\Lbf{1. Training:}\\
Table \ref{table:imitation_training} reports the binary cross-entropy training loss and accuracy of the cloned model trained on data generated from the expert.
The Adam optimizer was used with a learning rate of 0.00025.
Each cloned model was trained for 50 epochs with a batch size of 32.

Data was generated using different numbers of training samples, generated by recording the state and action of the expert.
The expert always completed the environment until time step 200, when it was terminated.
Thus each episode of training data consists of 200 individual state-action samples.

\begin{table}[h]
  \centering
  \begin{tabular}{| l | c | c | c | c |}
    \hline
    Episodes of Training Data & 1 & 10 & 50 & 100
    \\
    \hline
    Training Loss: & 0.401 & 0.250 & 0.962 & 0.079 \\
    \hline
    Accuracy: & 0.875 & 0.885 & 0.962 & 0.968 \\
    \hline
  \end{tabular}
  \captionof{table}{Loss and accuracy for final training epoch of cloned behavior}
  \label{table:imitation_training}
\end{table}


\Lbf{2. Evaluation on simple environment:}\\
Table \ref{table:imitation_evaluation_easy} reports the cumulative reward averaged over 50 episodes for the cloned behaviors and the Expert on the easier, unwrapped CartPole-v0.
In this environment the maximum reward ever seen is 200.
In this simple environment the cloned behavior with only 1 episode of training data performs worse than the expert, but all other cloned behaviors are able to balance the pendulum to receive the maximum reward.

\begin{table}[h]
  \centering
  \begin{tabular}{| l | c | c | c | c | c |}
    \hline
    Episodes of Training Data & 1 & 10 & 50 & 100 & Expert
    \\
    \hline
    Reward: & 100.18 \pm 33.09 & 200 \pm 0.0 & 200 \pm 0.0 & 200 \pm 0.0 & 200 \pm 0.0\\
    \hline
  \end{tabular}
  \captionof{table}{Cumulative reward for the cloned models and the expert averaged over 50 trails in the basic (unwrapped) CartPole-v0 environment}
  \label{table:imitation_evaluation_easy}
\end{table}


\Lbf{3. Evaluation on the hard environment:}\\
Table \ref{table:imitation_evaluation_hard} reports the cumulative reward averaged over 50 episodes for the cloned behaviors and the Expert on the harder, wrapped CartPole-v0.
In this harder environment all behaviors perform worse compared to the easier environment.
While there is a large improvement for training the cloned behavior on 10 episodes compared to 1 episode, additional training does not help (in the reported trail the averaged total reward actually decreased slightly).
None of the cloned behaviors match the reward of the expert.

\begin{table}[h]
  \centering
  \begin{tabular}{| l | c | c | c | c | c |}
    \hline
    Episodes of Training Data & 1 & 10 & 50 & 100 & Expert
    \\
    \hline
    Reward: & 19.82 \pm 17.62 & 64.80 \pm 58.08 & 51.2 \pm 53.12 & 44.2 \pm 52.12 & 69.8 \pm 50.4 \\
    \hline
  \end{tabular}
  \captionof{table}{Cumulative reward for the cloned models and the expert averaged over 50 trails in the more difficult, wrapped CartPole-v0 environment}
  \label{table:imitation_evaluation_hard}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Problem 3}

\end{document}
